\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{Instance Segmentation for Urban Street Scenes}

\author{Francesco Bari\\
{\tt\small francesco.bari.2@studenti.unipd.it}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Eleonora Signor\\
{\tt\small eleonora.signor@studenti.unipd.it}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
In questo lavoro abbiamo confrontato differenti tecniche di instance segmentation, gi\`a esistenti, sul task specifico di \textit{Urban Street Scenes}. Il nostro interesse verso il topic \`e nato dal fatto che la segmentazione delle istanze \`e uno dei compiti fondamentali della visione, tuttavia si presenta ancora complesso e non del tutto esplorato. 
%Esistono differenti approcci di instance segmentation, di seguito ne presentiamo una sottoparte, valutandone accuracy e performance su dataset multicategoriali e rivolti a quantificare la robustezza di un algoritmo.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
La segmentazione dell'immagine \`e il processo di separazione di questa in pi\`u segmenti, in cui ciascun pixel viene associato a un tipo di oggetto. Esistono due tipologie di segmentazione dell'immagine: la segmentazione semantica e la segmentazione d'instanza. La prima contrassegna oggetti dello stesso tipo con la medesima etichetta di classe; la seconda contrassegna oggetti dello stesso tipo e appartenenti a entit\`a distinte con etichette di classe differenti. L'idea che abbiamo cercato di sviluppare ha riguardato il confronto di diverse metodoligie di instance segmentation. La prima tecnica che abbiamo studiato \`e stato Mask R-CNN~\cite{Authors1_maskrcnn}, approccio a due stadi. Questa l'abbiamo scelta alla luce del riscontro positivo che ha ricevuto dal mondo della vision research, grazie al suo framework concettualmente semplice e generale, caratterizzato da un rilevamento di oggetti d'immagine efficiente, e dalla generazione in contentemporanea di una maschera di segmentazione di alta qualit\`a per ogni istanza. La tecnica che abbiamo deciso di contraporre a Mask R-CNN~\cite{Authors1_maskrcnn} \`e stata BlendMask~\cite{Authors2_BlendMask}. Questa \`e invece una tecnica a uno stadio che si \`e presentata capace di superare le prestazioni di Mask R-CNN~\cite{Authors1_maskrcnn} sia a livello di previsione della maschera che per tempo di formazione, sui datasets MSCOCO 2017~\cite{Authors3_MSCOCO} e LVIS~\cite{Authors4_LVIS}. Ci siamo interessati a verificare se questo rimanesse valido anche su datasets, come \textit{Cityscapes}~\cite{cityscapes} e \textit{WildDash}~\cite{wildDash}, appartenenti allo specifico topic di \textit{Urban Street Scenes}, caraterizzati da immagini provenienti dalle strade di tutto il mondo, con molti scenari difficili. Alcuni aspetti che abbiamo testato hanno riguardato cambiamenti di backbone, profondit\`a della rete ResNet~\cite{Authors5_ResNet} e numero di layers congelati. I risultati ottenuti ci hanno confermato quanto gi\`a annunciato dai lavori precedenti, generalizzando BlendMask~\cite{Authors2_BlendMask} come uno degli approccio a stadi pi\`u promettenti. Al termine del nostro lavoro e per non limitare la nostra analisi abbiamo fatto qualche considerazione anche su altre tecniche di instance segmentation quali SOLOv2~\cite{Authors6_SOLOv2} e Deep Snake~\cite{Authors7_deepsnake}.

\section{Related Work}
\label{sec:related-work}
L'approccio che abbiamo usato nel nostro lavoro \`e stato utilizzare come linee giuda i papers ~\cite{Authors1_maskrcnn},~\cite{Authors2_BlendMask},~\cite{Authors6_SOLOv2} e~\cite{Authors7_deepsnake} estendendo le analisi anche su dataset di \textit{Urban Street Scenes}.
\subsection{Stage approch} 
Mask R-CNN~\cite{Authors1_maskrcnn} \`e una Rete Neurale Convoluzionale che si presenta all'avanguardia in termini di segmentazione dell'immagine. \`E la variante di una Rete Neurale Profonda che rileva gli oggetti di un'immagine e vi genera una maschera di segmentazione per ciascuna istanza.
Mask R-CNN~\cite{Authors1_maskrcnn} \`e l'evoluzione successiva di Faster R-CNN~\cite{fasterRCNN}, Rete Neurale Convoluzionale Region-based, che produce per ogni oggetto candidato 3 output: l'etichetta di classe, l'offset del riquadro di delimitazione e la maschera dell'oggetto.
L'architettura della rete, Figure \ref{fig:mask_rcnn}, si compone di una CNN (backbone), che processa l'immagine e estrae la feature map. Dopodich\`e grazie alla Region Proposal Network vengono presentate le proposte o RoI, sul quale andare a fare riconosciemento del riquadro di delimitazione e previsione delle maschera (head). Inoltre prima di generare l'ouput a ciascuna RoI viene applicato RoIAlign, che permette di ottenere una maschera dove il layout dell'oggetto viene mantenuto.
\begin{figure}[H]
\centering
  \includegraphics[width=0.6\linewidth]{./image/maskrcnn.png}
  \caption{Mask R-CNN architecture. \textit{Source:~\cite{fig1}, pag. 3.}}
  \label{fig:mask_rcnn}
\noindent
\end{figure}
\indent BlendMask~\cite{Authors2_BlendMask} deriva dai limiti di Mask R-CNN~\cite{Authors1_maskrcnn}. Gli autori del paper definiscono come Mask R-CNN~\cite{Authors1_maskrcnn} vincoli fortemente la velocit\`a e la qualit\`a di generazione delle maschere alle heads, facendo cos\`i fatica a trattare scenari complicati e ponendo un limite alla risoluzione delle maschere. Inoltre Mask R-CNN~\cite{Authors1_maskrcnn} si presenta come un framework poco flessibile per reti multi-task. Hanno cos\`i cercato di cobinare strategie di ricerca dall'alto verso il basso e dal basso verso l'alto in FCOS~\cite{fcos}, one stage approch, che sembra in grado di superare le controparti a due stadi in termini di precisione. L'architettura di BlendMask~\cite{Authors2_BlendMask}, Figure \ref{fig:blendmask}, si compone di un detector network e di una mask branch. Quest'ultima \`e partizionata in 3 parti: il modulo inferiore che si occupa di prevedere le scores map, chiamate basi; the top layer composto da un singolo strato di convoluzione e da torri, tante quante sono le input features, con il compito di predirre attention instance, e un modulo blender che unisce scores con attenzioni.
\begin{figure}[H]
\centering
  \includegraphics[width=1\linewidth]{./image/blendmask.png}
  \caption{BlendMask architecture. \textit{Source:~\cite{Authors2_BlendMask}, pag. 3.}}
  \label{fig:blendmask}
\noindent
\end{figure}
\noindent

\indent SOLOv2~\cite{Authors6_SOLOv2} \`e un altro approccio a singolo stage, sucessore di SOLO~\cite{solo}. In questo caso ogni istanza di un'immagine viene segmentata dinamicamente, senza rilevamento del riquadro di delimitazione.
La generazione della maschera, a differenza di Mask R-CNN~\cite{Authors1_maskrcnn}, \`e disaccoppiata in mask kernel prediction e in mask feature learning. Questi due elementi sono responsabili della generazione dei convolution kernels e delle feature maps. SOLOv2~\cite{Authors6_SOLOv2} riesce a ottenere risultati promettenti anche grazie all'uso della matrix non-maximum suppression (NMS) technique, che riduce le previsioni duplicate guadagnandone in minor overhead d'inferenza.
\subsection{Contour-based approach}
Deep Snake~\cite{Authors7_deepsnake} \`e un approccio basato su contorni, che implementa l'idea degli algoritmi snake con learning-based approach. Deep Snake~\cite{Authors7_deepsnake} si compone di una pipeline a due stadi: in un prima istanza vi \`e  una proposta di controno iniziale sull'oggetto di un'immagine; fatta successivamente seguire dall'uso di una Rete Neurale, che deforma iterativamente questa proposta, fino a farla combacire esattamente con i confini propri dell'oggetto. Per l'apprendimento della struttura delle features del contorno, gli autori del paper propongono l'uso della Circular Convolution.

\section{Datasets}
I datasets di \textit{Urban Street Scenes}, che abbiamo usato, appartengono a \textit{Cityscapes}~\cite{cityscapes} e \textit{WildDash}~\cite{wildDash}.
\subsection{Cityscapes}
\textit{Cityscapes}~\cite{cityscapes} \`e una suite di benchmark e un dataset su larga scala per semantic urban scene understanding. \`E adatto per apprendere e testare metodi pixel-level e instance-level semantic labeling. Le immagini di \textit{Cityscapes}~\cite{cityscapes} sono state create da un'insieme vasto e diversificato di sequenze video, registrate in 50 citt\`a diverse. Queste possono essere comprensive di annotazioni di alta qualit\`a, o/e di tipo grossolane; quest'ultime consentono di testare metodi che impiegano grandi volumi di dati debolmente etichettati. Le annotazioni contenute, fondamentali per la valutazione di un modello, sono di tipo poligonale.\\ 
Il dataset che abbiamo usato, appartenete a \textit{Cityscapes}~\cite{cityscapes}, si partiziona in due unit\`a: \textit{gtFine} e \textit{leftImg8} (Figure \ref{fig:image_gtfine}), che abbiamo utilizzato in coppia. \textit{gtFine} si compone di annotazioni fini per 3 475 immagini di train e val, e 1 525 immagini per test set. \textit{leftImg8} da immagini "row" di traffico urbano, con train set, test set e val set; per un totale di 5 000 immagini.
\begin{figure}[H]
\centering
  \includegraphics[width=0.45\linewidth]{./image/gtFine.png} \quad \includegraphics[width=0.45\linewidth]{./image/leftImg8.png}
  \caption{\textit{Cityscapes}~\cite{cityscapes} images: \textit{gtFine} to left and \textit{leftImg8} to right.}
  \label{fig:image_gtfine}
\noindent
\end{figure}

In Figura \ref{fig:class_definitions_city}, riportiamo le classi e il numero di occorrenze in \textit{gtFine train} e \textit{leftImg8 train}. Il numero di classi complessive sono 8.
\begin{figure}[H]
\centering
  \includegraphics[width=1\linewidth]{./image/city_class} 
  \caption{\textit{Cityscapes} dataset class definitions.}
  \label{fig:class_definitions_city}
\noindent
\end{figure}
Tuttavia i datasets di \textit{Cityscapes}~\cite{cityscapes} anche se includono diversi mesi e stagioni, sono sempre immagini scattate in buone condizioni metereologiche. Questo aspetto ci ha spinto ad analizzare il comportamento delle nostre tecniche di instance segmentation anche sul dataset \textit{WildDash}~\cite{wildDash}.

\subsection{WildDash}
\textit{WildDash}~\cite{wildDash} \`e una suite di benchmark e un dataset per la segmentazione semantica e d'instanza per il dominio automobilistico. Le immagini, contenute nei datasets, provengono da diverse fonti da tutto il mondo. Inoltre presentano scenari, quali pioggia, oscurit\`a, copertura stradale che sono delle vere e proprie challenge per il riconoscimento delle immagini. Questo consente di mettere in luce le carenze di una qualsiasi tecnica di instance segmentation.\\
Il dataset che abbiamo usato \`e \textit{public gt package} (Figure \ref{fig:image_wd}), composto da 4 256 immagini, rivolto appositamente a risolvere tasks di instance segmentation; non era tuttavia suddiviso in train, val e test set. Abbiamo di conseguenza deciso di suddividerlo manualmente, riservando 3 405 immagini come train set e 851 immagini come test set. Non abbiamo ritenuto necessario fare un'ulteriore partizione in val set, in modo da mantenere pi\`u immagini possibili nel addestramento; e assumendo che i nostri modelli, usando pesi gi\`a preadestrati su ImageNet~\cite{imagenet}, fossero sufficientemente accurati da non richiedere model selection.
\begin{figure}[H]
\centering
  \includegraphics[width=0.45\linewidth]{./image/wd_rain.jpg} \quad \includegraphics[width=0.45\linewidth]{./image/wd_desert.jpg}
  \caption{\textit{WildDash}~\cite{wildDash} images:  \textit{rain scenario} to left and \textit{road in the desert} to right.}
  \label{fig:image_wd}
\noindent
\end{figure}
In Figura \ref{fig:class_definitions_wd}, riportiamo le classi e il numero di occorrenze in \textit{public gt package train}. Il numero di classi complessive sono 13.
\begin{figure}[H]
\centering
  \includegraphics[width=1\linewidth]{./image/wd_class} 
  \caption{\textit{WildDash} dataset class definitions.}
  \label{fig:class_definitions_wd}
\noindent
\end{figure}






\section{Method}

\subsection{Architecture}

\subsection{Hyperparameters of configuration}

\subsection{Training with fine-tuning}
\paragraph{Mask R-CNN}
La loss utilizzata durante il training \`e la seguente
\begin{center} min(L) $=$ min(L$_{cls}$ + L$_{box}$ + L$_{mask}$) \end{center}
L$_{cls}$ is the classification loss, L$_{box}$ is the bounding-box loss and  L$_{mask}$ is the average binary cross-entropy loss.
\paragraph{BlendMask}
La loss utilizzata durante il training \`e la seguente
\begin{center} min(L) $=$ min(semantic loss)~\cite{Authors4_semanticloss}\end{center}.
\subsection{Evalutation and inference}

\subsection{Metrics}
\begin{itemize}
\item AP
\item numero di istanze, tempo :: accuracy visiva.confidence-threshold
\end{itemize}



\section{Experiments}
In questa sezione descriviamo gli esperimenti che abbiamo eseguito per testare  e valutare le tecniche oggetto di questo lavoro. Tali esperimenti gli abbiamo eseguiti al termine delle fasi di studio e codifica.

\subsection{Backbone}
\label{experiments:second_trial}
La seconda serie di esperimenti, che abbiamo compiuto, riguarda la definizione della backbone. Tutte le tecniche a stadi, oggetto del confronto, sono dotate del suddetto modulo inferiore, per cui ci \`e risultato semplice uniformare le scelte architetturali in modo da poter compiere una valutazione oggettiva. Le tecniche che abbiamo confrontato sono state Mask R-CNN e BlendMask.\\
Le configurazioni constanti delle reti sono image size ...,  numero massimo di iterazioni, learning rate ..., step size a ... e fine-tuning esclusivamente agli ultimi 2 livelli.

\begin{table}[H]
\scriptsize
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Method and architecture & \textit{Cityscapes} AP & \textit{WildDash} AP\\
\hline\hline
\begin{tabular}[c]{cc}Mask R-CNN $+$ ResNet50 \\ $+$ C4 $+$ Base-RCNN-C4\end{tabular} & & \\
\hline
\begin{tabular}[c]{cc}Mask R-CNN $+$ ResNet50 \\ $+$ DC5 $+$ Base-RCNN-DilatedC5\end{tabular} & & \\
\hline
\begin{tabular}[c]{cc}Mask R-CNN $+$ ResNet50 \\ $+$ FPN $+$ Base-RCNN-FPN\end{tabular} & &\\
\hline
\end{tabular}
\end{center}
\caption{Backbone Mask R-CNN result.}
\label{mytable_backbone_MaskRCNN}
\end{table}
\noindent
Per BlendMask, oltre a settare le configurazioni costanti, avvalendoci dei risultati presentati in ... abbiamo settato R $=$ 56, M $=$ 14, K $=$ 4, sampling method for bottom bases bilinear pooling, interpolation method for top-level attentions bilinear upsampling and semantic loss. Inoltre abbiamo deciso di testare vari tipi di decoder: ProtoNet and DeepLabv3+.
\begin{table}[H]
\scriptsize
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Method and architecture & \textit{Cityscapes} AP & \textit{WildDash} AP\\
\hline\hline
\begin{tabular}[c]{cc}BlendMask with decoder ProtoNet \\ $+$ ResNet50 $+$ FPN $+$ Base-550\end{tabular} & &\\
\hline
\begin{tabular}[c]{ccc}BlendMask with decoder ProtoNet \\ $+$ ResNet50 $+$ deformable convolution \\ $+$ FPN $+$ Base-550\end{tabular} & &\\
\hline
\begin{tabular}[c]{cc}BlendMask with decoder DeepLabv3+ \\ $+$ ResNet50 $+$ FPN $+$ Base-550\end{tabular} & &\\
\hline
\begin{tabular}[c]{ccc}BlendMask with decoder DeepLabv3+ \\ $+$ ResNet50 $+$ deformable convolution \\ $+$ FPN  $+$ Base-550\end{tabular} & &\\
\hline
\end{tabular}
\end{center}
\caption{Backbone BlendMask result.}
\label{mytable_backbone_BlendMask}
\end{table}

\subsection{Deepness}
Una terza serie di esperimenti ha riguardato lo studio della profondit\`a delle reti ResNet.\\
I parametri di configurazione non definiti in modo esplicito, sono le medesime di quelle riportate nella sezione \S\ref{experiments:second_trial}.
\begin{table}[H]
\scriptsize
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Method and architecture & \textit{Cityscapes} AP & \textit{WildDash} AP\\
\hline\hline
\begin{tabular}[c]{cc}BlendMask with decoder ProtoNet \\ $+$ ResNet101 $+$ FPN $+$ Base-BlendMask\end{tabular} & &\\
\hline
\begin{tabular}[c]{ccc}BlendMask with decoder ProtoNet \\ $+$ ResNet101 $+$ deformable convolution \\$+$ FPN $+$ Base-BlendMask\end{tabular} & &\\
\hline
\end{tabular}
\end{center}
\caption{Deepness BlendMask result.}
\label{mytable_deepness_BlendMask}
\end{table}

\subsection{Freeze levels}
Per la quarta serie di esperimenti ci siamo voluti concentrare sul numero di layers da "scongelare" di ResNet durante il re-training dei pesi.\\
I parametri di configurazione non definiti in modo esplicito, sono le medesime di quelle riportate nella sezione \S \ref{experiments:second_trial}.
\begin{table}[H]
\scriptsize
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Method and architecture & \textit{Cityscapes} AP & \textit{WildDash} AP\\
\hline\hline
\begin{tabular}[c]{ccc}Mask R-CNN $+$ ResNet101 $+$ FPN \\ 1 layers freeze\end{tabular} & &\\
\hline
\begin{tabular}[c]{ccc}Mask R-CNN $+$ ResNet101 $+$ FPN \\ 3 layers freeze\end{tabular} & &\\
\hline
\begin{tabular}[c]{ccc}BlendMask with decoder ProtoNet \\ $+$ ResNet101 $+$ FPN $+$ Base-BlendMask \\ 1 layers freeze\end{tabular} & &\\
\hline
\begin{tabular}[c]{ccc}BlendMask with decoder ProtoNet \\ $+$ ResNet101 $+$ FPN $+$ Base-BlendMask \\ 3 layers freeze\end{tabular} & &\\
\hline
\end{tabular}
\end{center}
\caption{Freeze layers result.}
\label{mytable_deepness_BlendMask}
\end{table}


\subsection{Own best models}
Come ultima serie di esperimenti abbiamo cercato di individuare i modelli migliori, per ciascuna le due tecniche di instance segmentation in esame in questa sezione; tenedo conto della possibilit\`a di allenare ciascun modello solo su una singolo macchina e 1 GPU.\\
I parametri di configurazione non definiti in modo esplicito, sono le medesime di quelle riportate nella sezione \S \ref{experiments:second_trial}.

\begin{table}[H]
\scriptsize
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Dataset & method and architecture & AP\\
\hline\hline

\hline
\end{tabular}
\end{center}
\caption{Own best models result.}
\label{mytable_own_best_model}
\end{table}



\section{Conclusion}
Dai nostri esperimenti siamo in grado di dire che le tecniche a uno stadio e box based, opportunamente modificate, funzionano meglio rispetto a metodi ha due stadi (di cui re \`e Mask R-CNN~\cite{Authors1_maskrcnn}),  sia in termini di AP precision che di tempo GPU. Tali effetti positivi sono probabilmente causati dall'ibridazione di metodi top down e bottom up, e dall'rilevamento degli oggetti senza ancoraggio (come accade per BlendMask~\cite{Authors2_BlendMask}).
In aggiunta, recenti studi riportati in~\cite{Authors6_SOLOv2} e in parte mostrati in Figure \ref{fig:conclusionSOLOv2}, provano che uno stage approch box-free combinato a matrix NMS, come lo \`e SOLOv2~\cite{Authors6_SOLOv2}, si dimostra molto competitivo nei confronti di BlendMask~\cite{Authors2_BlendMask}.
\begin{figure}[H]
\centering
  \includegraphics[width=0.7\linewidth]{./image/conclusion_SOLOv2.png}
  \caption{Comparison between SOLOv2 and other stage approches. \textit{Source:~\cite{Authors6_SOLOv2}, pag. 2.}}
  \label{fig:conclusionSOLOv2}
\noindent
\end{figure}
Inoltre gli stage approch non sono gli unici metodi possibili per risolvere tasks di istance segmentation. Per esempio esiste Deep Snake~\cite{Authors7_deepsnake}, counter based approch che supera Mask R-CNN~\cite{Authors1_maskrcnn} sia in termini di velocit\`a di inferenza che AP precision, come mostrato dalla nella tablella in Figure \ref{fig:conclusiondeepsnake}, e ha il potenziale per essere un buon competitor di SOLOv2~\cite{Authors6_SOLOv2}. Una possibile estensione futura, in questa direzione, pu\`o essere utilizzare l'approccio basato su contorni per il rilevamento dei riquadri di delimitazione in BlendMask~\cite{Authors2_BlendMask}.
\begin{figure}[H]
\centering
  \includegraphics[width=1\linewidth]{./image/conclusion_deepsnake.png}
  \caption{Results on \textit{Cityscapes} val (AP [val] column) and test (remaining columns) sets. \textit{Source:~\cite{Authors7_deepsnake}, pag. 7.}}
  \label{fig:conclusiondeepsnake}
\noindent
\end{figure}


\begin{thebibliography}{1}
\bibitem{Authors1_maskrcnn}
Kaiming He and Georgia Gkioxari and Piotr Dollár and Ross Girshick.
Mask R-CNN. CoRR, 2018.
\bibitem{Authors2_BlendMask}
Hao Chen, Kunyang Sun, Zhi Tian, Chunhua Shen, Yongming Huang and Youliang Yan.
BlendMask: Top-Down Meets Bottom-Up for Instance Segmentation. CoRR, 2020.
\bibitem{Authors3_MSCOCO}
Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\'ar and C. Lawrence Zitnick.
Microsoft COCO: Common Objects in Context. CoRR, 2014.
\bibitem{Authors4_LVIS}
Agrim Gupta, Piotr Doll\'ar and Ross B. Girshick.
LVIS: A Dataset for Large Vocabulary Instance Segmentation. CoRR, 2019.
\bibitem{cityscapes}
Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth and Bernt Schiele.
The Cityscapes Dataset for Semantic Urban Scene Understanding. CoRR, 2016.
\bibitem{wildDash}
Zendel, Oliver and Honauer, Katrin and Murschitz, Markus and Steininger, Daniel and Dominguez, Gustavo Fernandez.
WildDash - Creating Hazard-Aware Benchmarks. Proceedings of the European Conference on Computer Vision, (ECCV), 2018.
\bibitem{Authors5_ResNet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren and Jian Sun.
Deep Residual Learning for Image Recognition. CoRR, 2015.
\bibitem{Authors6_SOLOv2}
Xinlong Wang,Rufeng Zhang,Tao Kong, Lei Li and Chunhua Shen.
SOLOv2: Dynamic, Faster and Stronger. CoRR, 2020.
\bibitem{Authors7_deepsnake}
Sida Peng, Wen Jiang, Huaijin Pi, Hujun Bao and Xiaowei Zhou.
Deep Snake for Real-Time Instance Segmentation. CoRR, 2020.
\bibitem{fasterRCNN}
Shaoqing Ren, Kaiming He, Ross B. Girshick, Jian Sun.
Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. CoRR, 2015.
\bibitem{fig1}
Bienias, Lukasz \& n, Juanjo \& Nielsen, Line \& Alstrøm, Tommy. Insights Into The Behaviour Of Multi-Task Deep Neural Networks For Medical Image Segmentation. 2019.
\bibitem{fcos}
Zhi Tian, Chunhua Shen, Hao Chen and Tong He.
FCOS: Fully Convolutional One-Stage Object Detection. CoRR, 2019.
\bibitem{solo}
Xinlong Wang, Tao Kong, Chunhua Shen, Yuning Jiang and Lei Li
SOLO: Segmenting Objects by Locations. CoRR, 2019.
\bibitem{imagenet}
J. Deng, W. Dong, R. Socher, L. -J. Li, Kai Li and Li Fei-Fei, "ImageNet: A large-scale hierarchical image database," 2009 IEEE Conference on Computer Vision and Pattern Recognition, 2009, pp. 248-255, doi: 10.1109/CVPR.2009.5206848.
\bibitem{Authors8_semanticloss}
Xu, Jingyi and Zhang, Zilu and Friedman, Tal and Liang, Yitao and Van den Broeck, Guy.
A Semantic Loss Function for Deep Learning with Symbolic Knowledge. Proceedings of the 35th International Conference on Machine Learning, 2018.

\end{thebibliography}



\end{document}
