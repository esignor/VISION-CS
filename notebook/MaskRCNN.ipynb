{"cells":[{"cell_type":"markdown","metadata":{"id":"685PR7J_3j_e"},"source":["# Install Detectron2\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19704,"status":"ok","timestamp":1643452789506,"user":{"displayName":"Matteo De Marchi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04834726655050483734"},"user_tz":-60},"id":"1stco0Pag8IQ","outputId":"98a853c6-3190-489f-86bc-a39809bfa319"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"5j_c9Z5m2Z9V","executionInfo":{"status":"ok","timestamp":1643452814781,"user_tz":-60,"elapsed":25297,"user":{"displayName":"Matteo De Marchi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04834726655050483734"}}},"outputs":[],"source":["!pip install pyyaml==5.1\n","\n","from IPython.display import Image, clear_output\n","\n","# Cerchiamo la versione che porti il match perfetto tra le librerie\n","import torch\n","TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n","CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n","\n","!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/$CUDA_VERSION/torch$TORCH_VERSION/index.html\n","\n","clear_output()"]},{"cell_type":"markdown","metadata":{"id":"ddBDGzkF3ieA"},"source":["# Import Libraries\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"69WlrA843cgx","executionInfo":{"status":"ok","timestamp":1643452816119,"user_tz":-60,"elapsed":1359,"user":{"displayName":"Matteo De Marchi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04834726655050483734"}}},"outputs":[],"source":["# Setup detectron2 logger\n","import detectron2\n","from detectron2.utils.logger import setup_logger\n","setup_logger()\n","\n","# Common libraries\n","import numpy as np\n","import os, json, cv2, random\n","from google.colab.patches import cv2_imshow\n","\n","# Common detectron2 utilities\n","from detectron2 import model_zoo\n","from detectron2.engine import DefaultPredictor\n","from detectron2.config import get_cfg\n","from detectron2.utils.visualizer import Visualizer\n","from detectron2.data import MetadataCatalog, DatasetCatalog"]},{"cell_type":"markdown","metadata":{"id":"lNf3UxHK3_EZ"},"source":["# Import Model(s)\n","R101-FNP (Istance Segmentation)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"7luWhdLz4Us4","executionInfo":{"status":"ok","timestamp":1643452816123,"user_tz":-60,"elapsed":19,"user":{"displayName":"Matteo De Marchi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04834726655050483734"}}},"outputs":[],"source":["ResNet_101_FPN = \"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"\n","# Resnet_50_FPN = \"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_1x.yaml\""]},{"cell_type":"markdown","metadata":{"id":"I_EGdFTP45Fl"},"source":["Specifichiamo il file di configurazione"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20728,"status":"ok","timestamp":1643452836836,"user":{"displayName":"Matteo De Marchi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04834726655050483734"},"user_tz":-60},"id":"aPbc3HLs45gR","outputId":"bcdc5827-7a4f-43a3-9f02-5592d9507aa2"},"outputs":[{"output_type":"stream","name":"stderr","text":["model_final_a3ec72.pkl: 254MB [00:07, 34.4MB/s]                           \n"]}],"source":["cfg = get_cfg()\n","\n","# Prendiamo le config dal file presente nel model zoo\n","cfg.merge_from_file(model_zoo.get_config_file(ResNet_101_FPN))\n","\n","# Impostiamo una soglia sulle detection che andrà a scartare\n","cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n","\n","# Prendimao i pesi relativi ad un modello già addestrato su COCO 2017\n","cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(ResNet_101_FPN)\n","\n","# Bisogna aver attivato la GPU per eseguire \n","predictor = DefaultPredictor(cfg)"]},{"cell_type":"markdown","metadata":{"id":"lOIF7Zn0MhIU"},"source":["# Fine-Tuning"]},{"cell_type":"markdown","metadata":{"id":"sbOTGNBdQk42"},"source":["#Cityscapes"]},{"cell_type":"markdown","metadata":{"id":"KZMBlttvCCrX"},"source":["Importiamo script utili per Cityscapes"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"XiuTZXcj1RyC","executionInfo":{"status":"ok","timestamp":1643454028634,"user_tz":-60,"elapsed":1739,"user":{"displayName":"Matteo De Marchi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04834726655050483734"}}},"outputs":[],"source":["!python -m pip install cityscapesscripts\n","\n","clear_output()"]},{"cell_type":"markdown","metadata":{"id":"6HRsBpIzCI7I"},"source":["Implementiamo le funzioni di cityscapes.py un pelo modificate"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"oMVigCgt2NnB","colab":{"base_uri":"https://localhost:8080/","height":374},"executionInfo":{"status":"error","timestamp":1643454029441,"user_tz":-60,"elapsed":838,"user":{"displayName":"Matteo De Marchi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04834726655050483734"}},"outputId":"f3514ade-eb37-4d47-dbbd-db9684352bcc"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-3c1a01f7bdb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdetectron2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msetup_logger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcityscapesscripts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhelpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcityscapesscripts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhelpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mid2label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname2label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cityscapesscripts'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["import functools\n","import json\n","import logging\n","import multiprocessing as mp\n","import numpy as np\n","import os\n","from itertools import chain\n","import pycocotools.mask as mask_util\n","from PIL import Image\n","\n","from detectron2.structures import BoxMode\n","from detectron2.utils.comm import get_world_size\n","from detectron2.utils.file_io import PathManager\n","from detectron2.utils.logger import setup_logger\n","\n","from cityscapesscripts.helpers.labels import labels\n","from cityscapesscripts.helpers.labels import id2label, name2label\n","\n","try:\n","    import cv2\n","except ImportError:\n","    # OpenCV is an optional dependency at the moment\n","    pass\n","\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","def _get_cityscapes_files(image_dir, gt_dir):\n","    files = []\n","    # scan through the directory\n","    cities = PathManager.ls(image_dir)\n","    logger.info(f\"{len(cities)} cities found in '{image_dir}'.\")\n","    for city in cities:\n","        city_img_dir = os.path.join(image_dir, city)\n","        city_gt_dir = os.path.join(gt_dir, city)\n","        for basename in PathManager.ls(city_img_dir):\n","            image_file = os.path.join(city_img_dir, basename)\n","\n","            suffix = \"leftImg8bit.png\"\n","            assert basename.endswith(suffix), basename\n","            basename = basename[: -len(suffix)]\n","\n","            instance_file = os.path.join(city_gt_dir, basename + \"gtFine_instanceIds.png\")\n","            label_file = os.path.join(city_gt_dir, basename + \"gtFine_labelIds.png\")\n","            json_file = os.path.join(city_gt_dir, basename + \"gtFine_polygons.json\")\n","\n","            files.append((image_file, instance_file, label_file, json_file))\n","    assert len(files), \"No images found in {}\".format(image_dir)\n","    for f in files[0]:\n","        assert PathManager.isfile(f), f\n","    return files\n","\n","\n","def load_cityscapes_instances(image_dir, gt_dir, from_json=True, to_polygons=True):\n","    \"\"\"\n","    Args:\n","        image_dir (str): path to the raw dataset. e.g., \"~/cityscapes/leftImg8bit/train\".\n","        gt_dir (str): path to the raw annotations. e.g., \"~/cityscapes/gtFine/train\".\n","        from_json (bool): whether to read annotations from the raw json file or the png files.\n","        to_polygons (bool): whether to represent the segmentation as polygons\n","            (COCO's format) instead of masks (cityscapes's format).\n","\n","    Returns:\n","        list[dict]: a list of dicts in Detectron2 standard format. (See\n","        `Using Custom Datasets </tutorials/datasets.html>`_ )\n","    \"\"\"\n","    if from_json:\n","        assert to_polygons, (\n","            \"Cityscapes's json annotations are in polygon format. \"\n","            \"Converting to mask format is not supported now.\"\n","        )\n","    files = _get_cityscapes_files(image_dir, gt_dir)\n","\n","    logger.info(\"Preprocessing cityscapes annotations ...\")\n","    # This is still not fast: all workers will execute duplicate works and will\n","    # take up to 10m on a 8GPU server.\n","    pool = mp.Pool(processes=max(mp.cpu_count() // get_world_size() // 2, 4))\n","\n","    ret = pool.map(\n","        functools.partial(_cityscapes_files_to_dict, from_json=from_json, to_polygons=to_polygons),\n","        files,\n","    )\n","    logger.info(\"Loaded {} images from {}\".format(len(ret), image_dir))\n","\n","    # Map cityscape ids to contiguous ids\n","    from cityscapesscripts.helpers.labels import labels\n","\n","    labels = [l for l in labels if l.hasInstances and not l.ignoreInEval]\n","    dataset_id_to_contiguous_id = {l.id: idx for idx, l in enumerate(labels)}\n","    for dict_per_image in ret:\n","        for anno in dict_per_image[\"annotations\"]:\n","            anno[\"category_id\"] = dataset_id_to_contiguous_id[anno[\"category_id\"]]\n","    return ret\n","\n","\n","def load_cityscapes_semantic(image_dir, gt_dir):\n","    \"\"\"\n","    Args:\n","        image_dir (str): path to the raw dataset. e.g., \"~/cityscapes/leftImg8bit/train\".\n","        gt_dir (str): path to the raw annotations. e.g., \"~/cityscapes/gtFine/train\".\n","\n","    Returns:\n","        list[dict]: a list of dict, each has \"file_name\" and\n","            \"sem_seg_file_name\".\n","    \"\"\"\n","    ret = []\n","    # gt_dir is small and contain many small files. make sense to fetch to local first\n","    gt_dir = PathManager.get_local_path(gt_dir)\n","    for image_file, _, label_file, json_file in _get_cityscapes_files(image_dir, gt_dir):\n","        label_file = label_file.replace(\"labelIds\", \"labelTrainIds\")\n","\n","        with PathManager.open(json_file, \"r\") as f:\n","            jsonobj = json.load(f)\n","        ret.append(\n","            {\n","                \"file_name\": image_file,\n","                \"sem_seg_file_name\": label_file,\n","                \"height\": jsonobj[\"imgHeight\"],\n","                \"width\": jsonobj[\"imgWidth\"],\n","            }\n","        )\n","    assert len(ret), f\"No images found in {image_dir}!\"\n","    assert PathManager.isfile(\n","        ret[0][\"sem_seg_file_name\"]\n","    ), \"Please generate labelTrainIds.png with cityscapesscripts/preparation/createTrainIdLabelImgs.py\"  # noqa\n","    return ret\n","\n","\n","def _cityscapes_files_to_dict(files, from_json, to_polygons):\n","    \"\"\"\n","    Parse cityscapes annotation files to a instance segmentation dataset dict.\n","\n","    Args:\n","        files (tuple): consists of (image_file, instance_id_file, label_id_file, json_file)\n","        from_json (bool): whether to read annotations from the raw json file or the png files.\n","        to_polygons (bool): whether to represent the segmentation as polygons\n","            (COCO's format) instead of masks (cityscapes's format).\n","\n","    Returns:\n","        A dict in Detectron2 Dataset format.\n","    \"\"\"\n","    from cityscapesscripts.helpers.labels import id2label, name2label\n","\n","    image_file, instance_id_file, _, json_file = files\n","\n","    annos = []\n","\n","    if from_json:\n","        from shapely.geometry import MultiPolygon, Polygon\n","\n","        with PathManager.open(json_file, \"r\") as f:\n","            jsonobj = json.load(f)\n","        ret = {\n","            \"file_name\": image_file,\n","            \"image_id\": os.path.basename(image_file),\n","            \"height\": jsonobj[\"imgHeight\"],\n","            \"width\": jsonobj[\"imgWidth\"],\n","        }\n","\n","        # `polygons_union` contains the union of all valid polygons.\n","        polygons_union = Polygon()\n","\n","        # CityscapesScripts draw the polygons in sequential order\n","        # and each polygon *overwrites* existing ones. See\n","        # (https://github.com/mcordts/cityscapesScripts/blob/master/cityscapesscripts/preparation/json2instanceImg.py) # noqa\n","        # We use reverse order, and each polygon *avoids* early ones.\n","        # This will resolve the ploygon overlaps in the same way as CityscapesScripts.\n","        for obj in jsonobj[\"objects\"][::-1]:\n","            if \"deleted\" in obj:  # cityscapes data format specific\n","                continue\n","            label_name = obj[\"label\"]\n","\n","            try:\n","                label = name2label[label_name]\n","            except KeyError:\n","                if label_name.endswith(\"group\"):  # crowd area\n","                    label = name2label[label_name[: -len(\"group\")]]\n","                else:\n","                    raise\n","            if label.id < 0:  # cityscapes data format\n","                continue\n","\n","            # Cityscapes's raw annotations uses integer coordinates\n","            # Therefore +0.5 here\n","            poly_coord = np.asarray(obj[\"polygon\"], dtype=\"f4\") + 0.5\n","            # CityscapesScript uses PIL.ImageDraw.polygon to rasterize\n","            # polygons for evaluation. This function operates in integer space\n","            # and draws each pixel whose center falls into the polygon.\n","            # Therefore it draws a polygon which is 0.5 \"fatter\" in expectation.\n","            # We therefore dilate the input polygon by 0.5 as our input.\n","            poly = Polygon(poly_coord).buffer(0.5, resolution=4)\n","\n","            if not label.hasInstances or label.ignoreInEval:\n","                # even if we won't store the polygon it still contributes to overlaps resolution\n","                polygons_union = polygons_union.union(poly)\n","                continue\n","\n","            # Take non-overlapping part of the polygon\n","            poly_wo_overlaps = poly.difference(polygons_union)\n","            if poly_wo_overlaps.is_empty:\n","                continue\n","            polygons_union = polygons_union.union(poly)\n","\n","            anno = {}\n","            anno[\"iscrowd\"] = label_name.endswith(\"group\")\n","            anno[\"category_id\"] = label.id\n","\n","            if isinstance(poly_wo_overlaps, Polygon):\n","                poly_list = [poly_wo_overlaps]\n","            elif isinstance(poly_wo_overlaps, MultiPolygon):\n","                poly_list = poly_wo_overlaps.geoms\n","            else:\n","                raise NotImplementedError(\"Unknown geometric structure {}\".format(poly_wo_overlaps))\n","\n","            poly_coord = []\n","            for poly_el in poly_list:\n","                # COCO API can work only with exterior boundaries now, hence we store only them.\n","                # TODO: store both exterior and interior boundaries once other parts of the\n","                # codebase support holes in polygons.\n","                poly_coord.append(list(chain(*poly_el.exterior.coords)))\n","            anno[\"segmentation\"] = poly_coord\n","            (xmin, ymin, xmax, ymax) = poly_wo_overlaps.bounds\n","\n","            anno[\"bbox\"] = (xmin, ymin, xmax, ymax)\n","            anno[\"bbox_mode\"] = BoxMode.XYXY_ABS\n","\n","            annos.append(anno)\n","    else:\n","        # See also the official annotation parsing scripts at\n","        # https://github.com/mcordts/cityscapesScripts/blob/master/cityscapesscripts/evaluation/instances2dict.py  # noqa\n","        with PathManager.open(instance_id_file, \"rb\") as f:\n","            inst_image = np.asarray(Image.open(f), order=\"F\")\n","        # ids < 24 are stuff labels (filtering them first is about 5% faster)\n","        flattened_ids = np.unique(inst_image[inst_image >= 24])\n","\n","        ret = {\n","            \"file_name\": image_file,\n","            \"image_id\": os.path.basename(image_file),\n","            \"height\": inst_image.shape[0],\n","            \"width\": inst_image.shape[1],\n","        }\n","\n","        for instance_id in flattened_ids:\n","            # For non-crowd annotations, instance_id // 1000 is the label_id\n","            # Crowd annotations have <1000 instance ids\n","            label_id = instance_id // 1000 if instance_id >= 1000 else instance_id\n","            label = id2label[label_id]\n","            if not label.hasInstances or label.ignoreInEval:\n","                continue\n","\n","            anno = {}\n","            anno[\"iscrowd\"] = instance_id < 1000\n","            anno[\"category_id\"] = label.id\n","\n","            mask = np.asarray(inst_image == instance_id, dtype=np.uint8, order=\"F\")\n","\n","            inds = np.nonzero(mask)\n","            ymin, ymax = inds[0].min(), inds[0].max()\n","            xmin, xmax = inds[1].min(), inds[1].max()\n","            anno[\"bbox\"] = (xmin, ymin, xmax, ymax)\n","            if xmax <= xmin or ymax <= ymin:\n","                continue\n","            anno[\"bbox_mode\"] = BoxMode.XYXY_ABS\n","            if to_polygons:\n","                # This conversion comes from D4809743 and D5171122,\n","                # when Mask-RCNN was first developed.\n","                contours = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)[\n","                    -2\n","                ]\n","                polygons = [c.reshape(-1).tolist() for c in contours if len(c) >= 3]\n","                # opencv's can produce invalid polygons\n","                if len(polygons) == 0:\n","                    continue\n","                anno[\"segmentation\"] = polygons\n","            else:\n","                anno[\"segmentation\"] = mask_util.encode(mask[:, :, None])[0]\n","            annos.append(anno)\n","    ret[\"annotations\"] = annos\n","    return ret\n","\n"]},{"cell_type":"markdown","metadata":{"id":"OU0AdQ-VCdzi"},"source":["Registriamo i dataset per cityscapes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XuIDWP6e2s-0","executionInfo":{"status":"aborted","timestamp":1643454029424,"user_tz":-60,"elapsed":22,"user":{"displayName":"Matteo De Marchi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04834726655050483734"}}},"outputs":[],"source":["cityscapes_classes = [k.name for k in labels if k.hasInstances and not k.ignoreInEval]\n","\n","image_dir = \"/content/drive/MyDrive/datasets/cityscapes/leftImg8bit/\"\n","gt_dir = \"/content/drive/MyDrive/datasets/cityscapes/gtFine/\"\n","\n","for d in [\"train\"]:\n","  DatasetCatalog.register(\"cityscapes_\" + d, lambda x=image_dir+d, y=gt_dir+d: load_cityscapes_instances(x, y))\n","  MetadataCatalog.get(\"cityscapes_\" + d).set(thing_classes=cityscapes_classes, evaluator_type=\"coco\")\n","\n","for d in [\"val\"]:\n","  DatasetCatalog.register(\"cityscapes_\" + d, lambda x=image_dir+d, y=gt_dir+d: load_cityscapes_instances(x, y))\n","  MetadataCatalog.get(\"cityscapes_\" + d).set(thing_classes=cityscapes_classes, evaluator_type=\"coco\")\n","\n","for d in [\"test\"]:\n","  DatasetCatalog.register(\"cityscapes_\" + d, lambda x=image_dir+d, y=gt_dir+d: load_cityscapes_instances(x, y))\n","  MetadataCatalog.get(\"cityscapes_\" + d).set(thing_classes=cityscapes_classes, evaluator_type=\"coco\")\n"]},{"cell_type":"markdown","metadata":{"id":"NUVFZviGCk_R"},"source":["Fine-tuning vero e proprio !"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H2Q8HAlbsbvF","executionInfo":{"status":"aborted","timestamp":1643454029431,"user_tz":-60,"elapsed":29,"user":{"displayName":"Matteo De Marchi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04834726655050483734"}}},"outputs":[],"source":["from detectron2.engine import DefaultTrainer\n","from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n","\n","\n","cfg = get_cfg()\n","cfg.merge_from_file(model_zoo.get_config_file(ResNet_101_FPN))\n","cfg.DATASETS.TRAIN = (\"cityscapes_train\",)\n","cfg.DATASETS.TEST = ()\n","cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(ResNet_101_FPN)\n","\n","# default 512\n","cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n","\n","# default 16\n","cfg.SOLVER.IMS_PER_BATCH = 2\n","\n","# default 40.000\n","cfg.SOLVER.MAX_ITER = 200\n","cfg.SOLVER.STEPS = (60,)\n","\n","# Classi di Cityscapes\n","cfg.MODEL.ROI_HEADS.NUM_CLASSES = 8\n","\n","\n","os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n","trainer = DefaultTrainer(cfg)\n","trainer.resume_or_load(resume=False)\n","trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"BmZ0xheYQY1s"},"source":["Print training metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8SbIuGtTB-n8","executionInfo":{"status":"aborted","timestamp":1643454029437,"user_tz":-60,"elapsed":35,"user":{"displayName":"Matteo De Marchi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04834726655050483734"}}},"outputs":[],"source":["%load_ext tensorboard\n","%tensorboard --logdir output"]},{"cell_type":"markdown","metadata":{"id":"tOxDYIJdhExZ"},"source":["#Save Models & Evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"iAYSShiYig7H"},"outputs":[],"source":["cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n","cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7\n","predictor = DefaultPredictor(cfg)"]},{"cell_type":"markdown","metadata":{"id":"4vEItq6j07mN"},"source":["COCO Evaluator API per compiere test e stampare AP"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gWe8Fw9JmZR6","executionInfo":{"status":"ok","timestamp":1643384217010,"user_tz":-60,"elapsed":12858,"user":{"displayName":"Matteo De Marchi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04834726655050483734"}},"outputId":"40610945-a961-4765-f77f-146a8fbb0539","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/28 15:33:43 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n","\u001b[32m[01/28 15:34:34 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n","\u001b[32m[01/28 15:34:34 d2.data.common]: \u001b[0mSerializing 500 elements to byte tensors and concatenating them all ...\n","\u001b[32m[01/28 15:34:34 d2.data.common]: \u001b[0mSerialized dataset takes 12.86 MiB\n","\u001b[32m[01/28 15:34:34 d2.evaluation.evaluator]: \u001b[0mStart inference on 500 batches\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","/usr/local/lib/python3.7/dist-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  max_size = (max_size + (stride - 1)) // stride * stride\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[32m[01/28 15:34:37 d2.evaluation.evaluator]: \u001b[0mInference done 11/500. Dataloading: 0.0024 s/iter. Inference: 0.1528 s/iter. Eval: 0.0580 s/iter. Total: 0.2133 s/iter. ETA=0:01:44\n","\u001b[32m[01/28 15:34:42 d2.evaluation.evaluator]: \u001b[0mInference done 34/500. Dataloading: 0.0027 s/iter. Inference: 0.1476 s/iter. Eval: 0.0690 s/iter. Total: 0.2196 s/iter. ETA=0:01:42\n","\u001b[32m[01/28 15:34:48 d2.evaluation.evaluator]: \u001b[0mInference done 57/500. Dataloading: 0.0029 s/iter. Inference: 0.1491 s/iter. Eval: 0.0688 s/iter. Total: 0.2210 s/iter. ETA=0:01:37\n","\u001b[32m[01/28 15:34:53 d2.evaluation.evaluator]: \u001b[0mInference done 76/500. Dataloading: 0.0030 s/iter. Inference: 0.1499 s/iter. Eval: 0.0805 s/iter. Total: 0.2336 s/iter. ETA=0:01:39\n","\u001b[32m[01/28 15:34:58 d2.evaluation.evaluator]: \u001b[0mInference done 94/500. Dataloading: 0.0030 s/iter. Inference: 0.1500 s/iter. Eval: 0.0905 s/iter. Total: 0.2437 s/iter. ETA=0:01:38\n","\u001b[32m[01/28 15:35:03 d2.evaluation.evaluator]: \u001b[0mInference done 114/500. Dataloading: 0.0030 s/iter. Inference: 0.1500 s/iter. Eval: 0.0947 s/iter. Total: 0.2480 s/iter. ETA=0:01:35\n","\u001b[32m[01/28 15:35:08 d2.evaluation.evaluator]: \u001b[0mInference done 132/500. Dataloading: 0.0030 s/iter. Inference: 0.1502 s/iter. Eval: 0.1005 s/iter. Total: 0.2539 s/iter. ETA=0:01:33\n","\u001b[32m[01/28 15:35:13 d2.evaluation.evaluator]: \u001b[0mInference done 151/500. Dataloading: 0.0031 s/iter. Inference: 0.1503 s/iter. Eval: 0.1025 s/iter. Total: 0.2560 s/iter. ETA=0:01:29\n","\u001b[32m[01/28 15:35:19 d2.evaluation.evaluator]: \u001b[0mInference done 170/500. Dataloading: 0.0030 s/iter. Inference: 0.1502 s/iter. Eval: 0.1049 s/iter. Total: 0.2583 s/iter. ETA=0:01:25\n","\u001b[32m[01/28 15:35:24 d2.evaluation.evaluator]: \u001b[0mInference done 189/500. Dataloading: 0.0030 s/iter. Inference: 0.1503 s/iter. Eval: 0.1063 s/iter. Total: 0.2598 s/iter. ETA=0:01:20\n","\u001b[32m[01/28 15:35:29 d2.evaluation.evaluator]: \u001b[0mInference done 205/500. Dataloading: 0.0029 s/iter. Inference: 0.1505 s/iter. Eval: 0.1107 s/iter. Total: 0.2644 s/iter. ETA=0:01:17\n","\u001b[32m[01/28 15:35:34 d2.evaluation.evaluator]: \u001b[0mInference done 224/500. Dataloading: 0.0030 s/iter. Inference: 0.1506 s/iter. Eval: 0.1118 s/iter. Total: 0.2656 s/iter. ETA=0:01:13\n","\u001b[32m[01/28 15:35:39 d2.evaluation.evaluator]: \u001b[0mInference done 243/500. Dataloading: 0.0030 s/iter. Inference: 0.1506 s/iter. Eval: 0.1124 s/iter. Total: 0.2663 s/iter. ETA=0:01:08\n","\u001b[32m[01/28 15:35:44 d2.evaluation.evaluator]: \u001b[0mInference done 261/500. Dataloading: 0.0031 s/iter. Inference: 0.1506 s/iter. Eval: 0.1132 s/iter. Total: 0.2671 s/iter. ETA=0:01:03\n","\u001b[32m[01/28 15:35:50 d2.evaluation.evaluator]: \u001b[0mInference done 280/500. Dataloading: 0.0030 s/iter. Inference: 0.1506 s/iter. Eval: 0.1137 s/iter. Total: 0.2676 s/iter. ETA=0:00:58\n","\u001b[32m[01/28 15:35:55 d2.evaluation.evaluator]: \u001b[0mInference done 296/500. Dataloading: 0.0030 s/iter. Inference: 0.1508 s/iter. Eval: 0.1162 s/iter. Total: 0.2702 s/iter. ETA=0:00:55\n","\u001b[32m[01/28 15:36:00 d2.evaluation.evaluator]: \u001b[0mInference done 313/500. Dataloading: 0.0030 s/iter. Inference: 0.1509 s/iter. Eval: 0.1176 s/iter. Total: 0.2717 s/iter. ETA=0:00:50\n","\u001b[32m[01/28 15:36:05 d2.evaluation.evaluator]: \u001b[0mInference done 329/500. Dataloading: 0.0030 s/iter. Inference: 0.1511 s/iter. Eval: 0.1195 s/iter. Total: 0.2738 s/iter. ETA=0:00:46\n","\u001b[32m[01/28 15:36:10 d2.evaluation.evaluator]: \u001b[0mInference done 346/500. Dataloading: 0.0030 s/iter. Inference: 0.1512 s/iter. Eval: 0.1206 s/iter. Total: 0.2749 s/iter. ETA=0:00:42\n","\u001b[32m[01/28 15:36:15 d2.evaluation.evaluator]: \u001b[0mInference done 364/500. Dataloading: 0.0030 s/iter. Inference: 0.1512 s/iter. Eval: 0.1215 s/iter. Total: 0.2758 s/iter. ETA=0:00:37\n","\u001b[32m[01/28 15:36:20 d2.evaluation.evaluator]: \u001b[0mInference done 381/500. Dataloading: 0.0030 s/iter. Inference: 0.1512 s/iter. Eval: 0.1225 s/iter. Total: 0.2769 s/iter. ETA=0:00:32\n","\u001b[32m[01/28 15:36:25 d2.evaluation.evaluator]: \u001b[0mInference done 399/500. Dataloading: 0.0030 s/iter. Inference: 0.1512 s/iter. Eval: 0.1231 s/iter. Total: 0.2775 s/iter. ETA=0:00:28\n","\u001b[32m[01/28 15:36:31 d2.evaluation.evaluator]: \u001b[0mInference done 416/500. Dataloading: 0.0030 s/iter. Inference: 0.1513 s/iter. Eval: 0.1245 s/iter. Total: 0.2789 s/iter. ETA=0:00:23\n","\u001b[32m[01/28 15:36:36 d2.evaluation.evaluator]: \u001b[0mInference done 434/500. Dataloading: 0.0029 s/iter. Inference: 0.1513 s/iter. Eval: 0.1248 s/iter. Total: 0.2793 s/iter. ETA=0:00:18\n","\u001b[32m[01/28 15:36:41 d2.evaluation.evaluator]: \u001b[0mInference done 452/500. Dataloading: 0.0029 s/iter. Inference: 0.1514 s/iter. Eval: 0.1250 s/iter. Total: 0.2795 s/iter. ETA=0:00:13\n","\u001b[32m[01/28 15:36:46 d2.evaluation.evaluator]: \u001b[0mInference done 471/500. Dataloading: 0.0029 s/iter. Inference: 0.1514 s/iter. Eval: 0.1245 s/iter. Total: 0.2789 s/iter. ETA=0:00:08\n","\u001b[32m[01/28 15:36:51 d2.evaluation.evaluator]: \u001b[0mInference done 490/500. Dataloading: 0.0029 s/iter. Inference: 0.1513 s/iter. Eval: 0.1241 s/iter. Total: 0.2786 s/iter. ETA=0:00:02\n","\u001b[32m[01/28 15:36:54 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:02:17.993503 (0.278775 s / iter per device, on 1 devices)\n","\u001b[32m[01/28 15:36:54 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:14 (0.151348 s / iter per device, on 1 devices)\n","\u001b[32m[01/28 15:36:54 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n","\u001b[32m[01/28 15:36:54 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n","\u001b[32m[01/28 15:36:54 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n","Loading and preparing results...\n","DONE (t=0.01s)\n","creating index...\n","index created!\n","\u001b[32m[01/28 15:36:54 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n","\u001b[32m[01/28 15:36:54 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.27 seconds.\n","\u001b[32m[01/28 15:36:54 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n","\u001b[32m[01/28 15:36:54 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.04 seconds.\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.084\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.141\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.091\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.020\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.108\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.195\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.025\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.087\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.094\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.020\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.122\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.225\n","\u001b[32m[01/28 15:36:54 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n","|  AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n","|:-----:|:------:|:------:|:-----:|:------:|:------:|\n","| 8.377 | 14.145 | 9.092  | 1.965 | 10.783 | 19.469 |\n","\u001b[32m[01/28 15:36:54 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n","| category   | AP     | category   | AP    | category   | AP     |\n","|:-----------|:-------|:-----------|:------|:-----------|:-------|\n","| person     | 19.623 | rider      | 0.000 | car        | 38.450 |\n","| truck      | 0.000  | bus        | 0.000 | train      | 0.000  |\n","| motorcycle | 0.000  | bicycle    | 8.944 |            |        |\n","Loading and preparing results...\n","DONE (t=0.06s)\n","creating index...\n","index created!\n","\u001b[32m[01/28 15:36:55 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *segm*\n","\u001b[32m[01/28 15:36:56 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 1.11 seconds.\n","\u001b[32m[01/28 15:36:56 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n","\u001b[32m[01/28 15:36:56 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.04 seconds.\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.075\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.133\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.074\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.010\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.092\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.194\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.023\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.079\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.084\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.013\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.107\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.214\n","\u001b[32m[01/28 15:36:56 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for segm: \n","|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n","|:-----:|:------:|:------:|:-----:|:-----:|:------:|\n","| 7.535 | 13.347 | 7.390  | 0.979 | 9.203 | 19.440 |\n","\u001b[32m[01/28 15:36:56 d2.evaluation.coco_evaluation]: \u001b[0mPer-category segm AP: \n","| category   | AP     | category   | AP    | category   | AP     |\n","|:-----------|:-------|:-----------|:------|:-----------|:-------|\n","| person     | 17.207 | rider      | 0.000 | car        | 36.167 |\n","| truck      | 0.000  | bus        | 0.000 | train      | 0.000  |\n","| motorcycle | 0.000  | bicycle    | 6.909 |            |        |\n","OrderedDict([('bbox', {'AP': 8.37719552335415, 'AP50': 14.145020747870776, 'AP75': 9.092270811142596, 'APs': 1.9652045179749384, 'APm': 10.782741483658759, 'APl': 19.469331075938868, 'AP-person': 19.62262181630402, 'AP-rider': 0.0, 'AP-car': 38.45048224389894, 'AP-truck': 0.0, 'AP-bus': 0.0, 'AP-train': 0.0, 'AP-motorcycle': 0.0, 'AP-bicycle': 8.944460126630238}), ('segm', {'AP': 7.535392787958278, 'AP50': 13.347106291952745, 'AP75': 7.390202111346916, 'APs': 0.978896678456102, 'APm': 9.2029791388561, 'APl': 19.439702064908438, 'AP-person': 17.20717637636843, 'AP-rider': 0.0, 'AP-car': 36.16735053986993, 'AP-truck': 0.0, 'AP-bus': 0.0, 'AP-train': 0.0, 'AP-motorcycle': 0.0, 'AP-bicycle': 6.908615387427861})])\n"]}],"source":["from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n","from detectron2.data import build_detection_test_loader\n","evaluator = COCOEvaluator(\"cityscapes_val\", cfg, False, output_dir=\"./output\")\n","val_loader = build_detection_test_loader(cfg, \"cityscapes_val\")\n","print(inference_on_dataset(predictor.model, val_loader, evaluator))"]},{"cell_type":"markdown","metadata":{"id":"n-WQenZrhaX4"},"source":["# WildDash"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"NNIllwzohfOi","executionInfo":{"status":"ok","timestamp":1643452836838,"user_tz":-60,"elapsed":16,"user":{"displayName":"Matteo De Marchi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04834726655050483734"}}},"outputs":[],"source":["from detectron2.data.datasets import register_coco_instances\n","register_coco_instances(\"dash_train\", {}, \"/content/drive/MyDrive/datasets/wd_public_02/wilddash_train.json\", \"/content/drive/MyDrive/datasets/wd_public_02/images_train\")\n","\n","\n","from detectron2.data.datasets import register_coco_instances\n","register_coco_instances(\"dash_test\", {}, \"/content/drive/MyDrive/datasets/wd_public_02/wilddash_test.json\", \"/content/drive/MyDrive/datasets/wd_public_02/images_test\")"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"Usp40drehn_7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643454763388,"user_tz":-60,"elapsed":410802,"user":{"displayName":"Matteo De Marchi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04834726655050483734"}},"outputId":"2a15bd81-f9bd-49c6-f847-5db906da1ebd"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[32m[01/29 11:05:53 d2.engine.defaults]: \u001b[0mModel:\n","GeneralizedRCNN(\n","  (backbone): FPN(\n","    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (top_block): LastLevelMaxPool()\n","    (bottom_up): ResNet(\n","      (stem): BasicStem(\n","        (conv1): Conv2d(\n","          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","        )\n","      )\n","      (res2): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","        )\n","      )\n","      (res3): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","        (3): BottleneckBlock(\n","          (conv1): Conv2d(\n","            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","      )\n","      (res4): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (3): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (4): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (5): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (6): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (7): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (8): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (9): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (10): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (11): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (12): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (13): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (14): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (15): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (16): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (17): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (18): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (19): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (20): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (21): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (22): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","      )\n","      (res5): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (proposal_generator): RPN(\n","    (rpn_head): StandardRPNHead(\n","      (conv): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n","      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","    (anchor_generator): DefaultAnchorGenerator(\n","      (cell_anchors): BufferList()\n","    )\n","  )\n","  (roi_heads): StandardROIHeads(\n","    (box_pooler): ROIPooler(\n","      (level_poolers): ModuleList(\n","        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n","        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n","        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n","        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n","      )\n","    )\n","    (box_head): FastRCNNConvFCHead(\n","      (flatten): Flatten(start_dim=1, end_dim=-1)\n","      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n","      (fc_relu1): ReLU()\n","      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n","      (fc_relu2): ReLU()\n","    )\n","    (box_predictor): FastRCNNOutputLayers(\n","      (cls_score): Linear(in_features=1024, out_features=14, bias=True)\n","      (bbox_pred): Linear(in_features=1024, out_features=52, bias=True)\n","    )\n","    (mask_pooler): ROIPooler(\n","      (level_poolers): ModuleList(\n","        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n","        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n","        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n","        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n","      )\n","    )\n","    (mask_head): MaskRCNNConvUpsampleHead(\n","      (mask_fcn1): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (mask_fcn2): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (mask_fcn3): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (mask_fcn4): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n","      (deconv_relu): ReLU()\n","      (predictor): Conv2d(256, 13, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","  )\n",")\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/29 11:05:54 d2.data.datasets.coco]: \u001b[0m\n","Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n","\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/29 11:05:54 d2.data.datasets.coco]: \u001b[0m/content/drive/MyDrive/datasets/wd_public_02/wilddash_train.json contains 31905 annotations, but only 31902 of them match to images in the file.\n","\u001b[32m[01/29 11:05:54 d2.data.datasets.coco]: \u001b[0mLoaded 3366 images in COCO format from /content/drive/MyDrive/datasets/wd_public_02/wilddash_train.json\n","\u001b[32m[01/29 11:05:54 d2.data.build]: \u001b[0mRemoved 32 images with no usable annotations. 3334 images left.\n","\u001b[32m[01/29 11:05:54 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n","\u001b[32m[01/29 11:05:54 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n","\u001b[32m[01/29 11:05:54 d2.data.common]: \u001b[0mSerializing 3334 elements to byte tensors and concatenating them all ...\n","\u001b[32m[01/29 11:05:54 d2.data.common]: \u001b[0mSerialized dataset takes 20.14 MiB\n"]},{"output_type":"stream","name":"stderr","text":["Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (14, 1024) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (14,) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (52, 1024) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (52,) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (13, 256, 1, 1) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (13,) in the model! You might want to double check if this is expected.\n","Some model parameters or buffers are not found in the checkpoint:\n","\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n","\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n","\u001b[34mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[32m[01/29 11:05:54 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/detectron2/data/detection_utils.py:433: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n","  torch.stack([torch.from_numpy(np.ascontiguousarray(x)) for x in masks])\n","/usr/local/lib/python3.7/dist-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  max_size = (max_size + (stride - 1)) // stride * stride\n","/usr/local/lib/python3.7/dist-packages/detectron2/data/detection_utils.py:433: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n","  torch.stack([torch.from_numpy(np.ascontiguousarray(x)) for x in masks])\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[32m[01/29 11:06:12 d2.utils.events]: \u001b[0m eta: 0:06:26  iter: 19  total_loss: 4.369  loss_cls: 2.557  loss_box_reg: 0.8576  loss_mask: 0.692  loss_rpn_cls: 0.05647  loss_rpn_loc: 0.1638  time: 0.8556  data_time: 0.1256  lr: 9.7405e-06  max_mem: 3947M\n","\u001b[32m[01/29 11:06:28 d2.utils.events]: \u001b[0m eta: 0:06:07  iter: 39  total_loss: 4.109  loss_cls: 2.353  loss_box_reg: 0.8219  loss_mask: 0.6872  loss_rpn_cls: 0.05066  loss_rpn_loc: 0.1116  time: 0.8198  data_time: 0.0206  lr: 1.9731e-05  max_mem: 3947M\n","\u001b[32m[01/29 11:06:44 d2.utils.events]: \u001b[0m eta: 0:05:51  iter: 59  total_loss: 3.619  loss_cls: 1.946  loss_box_reg: 0.8404  loss_mask: 0.6714  loss_rpn_cls: 0.0432  loss_rpn_loc: 0.08607  time: 0.8092  data_time: 0.0383  lr: 2.972e-05  max_mem: 3947M\n","\u001b[32m[01/29 11:07:00 d2.utils.events]: \u001b[0m eta: 0:05:35  iter: 79  total_loss: 3.041  loss_cls: 1.352  loss_box_reg: 0.8247  loss_mask: 0.6522  loss_rpn_cls: 0.03976  loss_rpn_loc: 0.1336  time: 0.8076  data_time: 0.0262  lr: 3.9711e-05  max_mem: 3947M\n","\u001b[32m[01/29 11:07:17 d2.utils.events]: \u001b[0m eta: 0:05:20  iter: 99  total_loss: 2.649  loss_cls: 0.9607  loss_box_reg: 0.8077  loss_mask: 0.6309  loss_rpn_cls: 0.05681  loss_rpn_loc: 0.1594  time: 0.8179  data_time: 0.0717  lr: 4.9701e-05  max_mem: 3947M\n","\u001b[32m[01/29 11:07:35 d2.utils.events]: \u001b[0m eta: 0:05:05  iter: 119  total_loss: 2.584  loss_cls: 0.904  loss_box_reg: 0.8408  loss_mask: 0.6165  loss_rpn_cls: 0.03499  loss_rpn_loc: 0.1636  time: 0.8333  data_time: 0.1341  lr: 5.9691e-05  max_mem: 3947M\n","\u001b[32m[01/29 11:07:50 d2.utils.events]: \u001b[0m eta: 0:04:49  iter: 139  total_loss: 2.452  loss_cls: 0.8162  loss_box_reg: 0.8314  loss_mask: 0.5746  loss_rpn_cls: 0.03543  loss_rpn_loc: 0.1709  time: 0.8223  data_time: 0.0196  lr: 6.9681e-05  max_mem: 3947M\n","\u001b[32m[01/29 11:08:06 d2.utils.events]: \u001b[0m eta: 0:04:33  iter: 159  total_loss: 2.39  loss_cls: 0.7566  loss_box_reg: 0.8676  loss_mask: 0.5574  loss_rpn_cls: 0.03085  loss_rpn_loc: 0.117  time: 0.8155  data_time: 0.0180  lr: 7.9671e-05  max_mem: 3947M\n","\u001b[32m[01/29 11:08:22 d2.utils.events]: \u001b[0m eta: 0:04:17  iter: 179  total_loss: 2.298  loss_cls: 0.6768  loss_box_reg: 0.774  loss_mask: 0.5112  loss_rpn_cls: 0.07664  loss_rpn_loc: 0.1848  time: 0.8146  data_time: 0.0497  lr: 8.966e-05  max_mem: 3947M\n","\u001b[32m[01/29 11:08:38 d2.utils.events]: \u001b[0m eta: 0:04:01  iter: 199  total_loss: 2.11  loss_cls: 0.6448  loss_box_reg: 0.809  loss_mask: 0.5085  loss_rpn_cls: 0.04124  loss_rpn_loc: 0.1445  time: 0.8117  data_time: 0.0303  lr: 9.9651e-05  max_mem: 3947M\n","\u001b[32m[01/29 11:08:54 d2.utils.events]: \u001b[0m eta: 0:03:45  iter: 219  total_loss: 2.237  loss_cls: 0.7064  loss_box_reg: 0.7749  loss_mask: 0.5177  loss_rpn_cls: 0.04999  loss_rpn_loc: 0.1638  time: 0.8119  data_time: 0.0454  lr: 0.00010964  max_mem: 3947M\n","\u001b[32m[01/29 11:09:10 d2.utils.events]: \u001b[0m eta: 0:03:29  iter: 239  total_loss: 2.083  loss_cls: 0.6698  loss_box_reg: 0.7797  loss_mask: 0.4768  loss_rpn_cls: 0.03151  loss_rpn_loc: 0.1607  time: 0.8124  data_time: 0.0479  lr: 0.00011963  max_mem: 3947M\n","\u001b[32m[01/29 11:09:26 d2.utils.events]: \u001b[0m eta: 0:03:14  iter: 259  total_loss: 2.005  loss_cls: 0.554  loss_box_reg: 0.7436  loss_mask: 0.45  loss_rpn_cls: 0.03582  loss_rpn_loc: 0.1308  time: 0.8111  data_time: 0.0151  lr: 0.00012962  max_mem: 3947M\n","\u001b[32m[01/29 11:09:44 d2.utils.events]: \u001b[0m eta: 0:02:57  iter: 279  total_loss: 2.014  loss_cls: 0.5809  loss_box_reg: 0.7262  loss_mask: 0.4409  loss_rpn_cls: 0.04446  loss_rpn_loc: 0.1919  time: 0.8167  data_time: 0.1121  lr: 0.00013961  max_mem: 3947M\n","\u001b[32m[01/29 11:10:01 d2.utils.events]: \u001b[0m eta: 0:02:41  iter: 299  total_loss: 1.74  loss_cls: 0.5105  loss_box_reg: 0.6637  loss_mask: 0.3898  loss_rpn_cls: 0.02944  loss_rpn_loc: 0.1412  time: 0.8170  data_time: 0.0634  lr: 0.0001496  max_mem: 3947M\n","\u001b[32m[01/29 11:10:16 d2.utils.events]: \u001b[0m eta: 0:02:25  iter: 319  total_loss: 1.714  loss_cls: 0.4748  loss_box_reg: 0.6717  loss_mask: 0.3827  loss_rpn_cls: 0.0309  loss_rpn_loc: 0.1384  time: 0.8144  data_time: 0.0158  lr: 0.00015959  max_mem: 3947M\n","\u001b[32m[01/29 11:10:32 d2.utils.events]: \u001b[0m eta: 0:02:09  iter: 339  total_loss: 1.917  loss_cls: 0.5119  loss_box_reg: 0.649  loss_mask: 0.3864  loss_rpn_cls: 0.03605  loss_rpn_loc: 0.2138  time: 0.8143  data_time: 0.0318  lr: 0.00016958  max_mem: 3947M\n","\u001b[32m[01/29 11:10:49 d2.utils.events]: \u001b[0m eta: 0:01:53  iter: 359  total_loss: 1.795  loss_cls: 0.5263  loss_box_reg: 0.6662  loss_mask: 0.389  loss_rpn_cls: 0.04425  loss_rpn_loc: 0.2106  time: 0.8146  data_time: 0.0329  lr: 0.00017957  max_mem: 3947M\n","\u001b[32m[01/29 11:11:05 d2.utils.events]: \u001b[0m eta: 0:01:37  iter: 379  total_loss: 1.47  loss_cls: 0.4328  loss_box_reg: 0.5366  loss_mask: 0.3428  loss_rpn_cls: 0.03461  loss_rpn_loc: 0.1039  time: 0.8134  data_time: 0.0199  lr: 0.00018956  max_mem: 3947M\n","\u001b[32m[01/29 11:11:21 d2.utils.events]: \u001b[0m eta: 0:01:21  iter: 399  total_loss: 1.556  loss_cls: 0.3919  loss_box_reg: 0.581  loss_mask: 0.3489  loss_rpn_cls: 0.03513  loss_rpn_loc: 0.1306  time: 0.8135  data_time: 0.0183  lr: 0.00019955  max_mem: 3947M\n","\u001b[32m[01/29 11:11:37 d2.utils.events]: \u001b[0m eta: 0:01:04  iter: 419  total_loss: 1.581  loss_cls: 0.427  loss_box_reg: 0.5735  loss_mask: 0.3456  loss_rpn_cls: 0.0452  loss_rpn_loc: 0.1622  time: 0.8119  data_time: 0.0305  lr: 0.00020954  max_mem: 3947M\n","\u001b[32m[01/29 11:11:53 d2.utils.events]: \u001b[0m eta: 0:00:48  iter: 439  total_loss: 1.6  loss_cls: 0.4507  loss_box_reg: 0.596  loss_mask: 0.3597  loss_rpn_cls: 0.04095  loss_rpn_loc: 0.1428  time: 0.8117  data_time: 0.0379  lr: 0.00021953  max_mem: 3947M\n","\u001b[32m[01/29 11:12:09 d2.utils.events]: \u001b[0m eta: 0:00:32  iter: 459  total_loss: 1.449  loss_cls: 0.3537  loss_box_reg: 0.5004  loss_mask: 0.3257  loss_rpn_cls: 0.04503  loss_rpn_loc: 0.16  time: 0.8111  data_time: 0.0171  lr: 0.00022952  max_mem: 3947M\n","\u001b[32m[01/29 11:12:26 d2.utils.events]: \u001b[0m eta: 0:00:16  iter: 479  total_loss: 1.413  loss_cls: 0.4083  loss_box_reg: 0.5589  loss_mask: 0.3131  loss_rpn_cls: 0.02947  loss_rpn_loc: 0.1061  time: 0.8131  data_time: 0.0711  lr: 0.00023951  max_mem: 3947M\n","\u001b[32m[01/29 11:12:42 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 499  total_loss: 1.415  loss_cls: 0.3689  loss_box_reg: 0.4687  loss_mask: 0.2673  loss_rpn_cls: 0.03047  loss_rpn_loc: 0.1729  time: 0.8113  data_time: 0.0180  lr: 0.0002495  max_mem: 3947M\n","\u001b[32m[01/29 11:12:43 d2.engine.hooks]: \u001b[0mOverall training speed: 498 iterations in 0:06:44 (0.8113 s / it)\n","\u001b[32m[01/29 11:12:43 d2.engine.hooks]: \u001b[0mTotal training time: 0:06:45 (0:00:01 on hooks)\n"]}],"source":["from detectron2.engine import DefaultTrainer\n","\n","cfg = get_cfg()\n","cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"))\n","cfg.DATASETS.TRAIN = (\"dash_train\",)\n","cfg.INPUT.MASK_FORMAT='bitmask'\n","cfg.DATASETS.TEST = ()\n","cfg.DATALOADER.NUM_WORKERS = 2\n","cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\")  # Let training initialize from model zoo\n","cfg.SOLVER.IMS_PER_BATCH = 2\n","cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\n","cfg.SOLVER.MAX_ITER = 500    # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset\n","cfg.SOLVER.STEPS = []        # do not decay learning rate\n","cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset (default: 512)\n","# Classi di WildDash\n","cfg.MODEL.ROI_HEADS.NUM_CLASSES = 13\n","\n","os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n","trainer = DefaultTrainer(cfg) \n","trainer.resume_or_load(resume=False)\n","trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"5eJI1smdQujG"},"source":["#Save Models & Evaluate"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"IMpFgNObDH1S","executionInfo":{"status":"ok","timestamp":1643454764638,"user_tz":-60,"elapsed":1278,"user":{"displayName":"Matteo De Marchi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04834726655050483734"}}},"outputs":[],"source":["cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n","cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7\n","predictor = DefaultPredictor(cfg)"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"QfNin8NGDMpH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643454930017,"user_tz":-60,"elapsed":165388,"user":{"displayName":"Matteo De Marchi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04834726655050483734"}},"outputId":"9f766365-7291-4218-8672-c3ae1d700812"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/29 11:12:44 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[01/29 11:12:44 d2.data.datasets.coco]: \u001b[0m\n","Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n","\n","\u001b[32m[01/29 11:12:44 d2.data.datasets.coco]: \u001b[0mLoaded 890 images in COCO format from /content/drive/MyDrive/datasets/wd_public_02/wilddash_test.json\n","\u001b[32m[01/29 11:12:44 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n","\u001b[32m[01/29 11:12:44 d2.data.common]: \u001b[0mSerializing 890 elements to byte tensors and concatenating them all ...\n","\u001b[32m[01/29 11:12:44 d2.data.common]: \u001b[0mSerialized dataset takes 5.77 MiB\n","\u001b[32m[01/29 11:12:44 d2.evaluation.evaluator]: \u001b[0mStart inference on 890 batches\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n","  max_size = (max_size + (stride - 1)) // stride * stride\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[32m[01/29 11:12:47 d2.evaluation.evaluator]: \u001b[0mInference done 11/890. Dataloading: 0.0015 s/iter. Inference: 0.1578 s/iter. Eval: 0.0124 s/iter. Total: 0.1717 s/iter. ETA=0:02:30\n","\u001b[32m[01/29 11:12:52 d2.evaluation.evaluator]: \u001b[0mInference done 39/890. Dataloading: 0.0019 s/iter. Inference: 0.1576 s/iter. Eval: 0.0201 s/iter. Total: 0.1797 s/iter. ETA=0:02:32\n","\u001b[32m[01/29 11:12:57 d2.evaluation.evaluator]: \u001b[0mInference done 67/890. Dataloading: 0.0020 s/iter. Inference: 0.1578 s/iter. Eval: 0.0202 s/iter. Total: 0.1801 s/iter. ETA=0:02:28\n","\u001b[32m[01/29 11:13:02 d2.evaluation.evaluator]: \u001b[0mInference done 94/890. Dataloading: 0.0020 s/iter. Inference: 0.1578 s/iter. Eval: 0.0218 s/iter. Total: 0.1817 s/iter. ETA=0:02:24\n","\u001b[32m[01/29 11:13:07 d2.evaluation.evaluator]: \u001b[0mInference done 122/890. Dataloading: 0.0020 s/iter. Inference: 0.1576 s/iter. Eval: 0.0212 s/iter. Total: 0.1810 s/iter. ETA=0:02:18\n","\u001b[32m[01/29 11:13:12 d2.evaluation.evaluator]: \u001b[0mInference done 151/890. Dataloading: 0.0020 s/iter. Inference: 0.1575 s/iter. Eval: 0.0207 s/iter. Total: 0.1803 s/iter. ETA=0:02:13\n","\u001b[32m[01/29 11:13:17 d2.evaluation.evaluator]: \u001b[0mInference done 179/890. Dataloading: 0.0020 s/iter. Inference: 0.1576 s/iter. Eval: 0.0209 s/iter. Total: 0.1806 s/iter. ETA=0:02:08\n","\u001b[32m[01/29 11:13:22 d2.evaluation.evaluator]: \u001b[0mInference done 207/890. Dataloading: 0.0020 s/iter. Inference: 0.1578 s/iter. Eval: 0.0211 s/iter. Total: 0.1810 s/iter. ETA=0:02:03\n","\u001b[32m[01/29 11:13:27 d2.evaluation.evaluator]: \u001b[0mInference done 232/890. Dataloading: 0.0020 s/iter. Inference: 0.1581 s/iter. Eval: 0.0230 s/iter. Total: 0.1832 s/iter. ETA=0:02:00\n","\u001b[32m[01/29 11:13:32 d2.evaluation.evaluator]: \u001b[0mInference done 257/890. Dataloading: 0.0020 s/iter. Inference: 0.1583 s/iter. Eval: 0.0245 s/iter. Total: 0.1850 s/iter. ETA=0:01:57\n","\u001b[32m[01/29 11:13:37 d2.evaluation.evaluator]: \u001b[0mInference done 285/890. Dataloading: 0.0020 s/iter. Inference: 0.1583 s/iter. Eval: 0.0244 s/iter. Total: 0.1849 s/iter. ETA=0:01:51\n","\u001b[32m[01/29 11:13:43 d2.evaluation.evaluator]: \u001b[0mInference done 313/890. Dataloading: 0.0020 s/iter. Inference: 0.1583 s/iter. Eval: 0.0242 s/iter. Total: 0.1846 s/iter. ETA=0:01:46\n","\u001b[32m[01/29 11:13:48 d2.evaluation.evaluator]: \u001b[0mInference done 341/890. Dataloading: 0.0020 s/iter. Inference: 0.1583 s/iter. Eval: 0.0241 s/iter. Total: 0.1845 s/iter. ETA=0:01:41\n","\u001b[32m[01/29 11:13:53 d2.evaluation.evaluator]: \u001b[0mInference done 369/890. Dataloading: 0.0020 s/iter. Inference: 0.1582 s/iter. Eval: 0.0240 s/iter. Total: 0.1843 s/iter. ETA=0:01:36\n","\u001b[32m[01/29 11:13:58 d2.evaluation.evaluator]: \u001b[0mInference done 397/890. Dataloading: 0.0020 s/iter. Inference: 0.1582 s/iter. Eval: 0.0237 s/iter. Total: 0.1840 s/iter. ETA=0:01:30\n","\u001b[32m[01/29 11:14:03 d2.evaluation.evaluator]: \u001b[0mInference done 424/890. Dataloading: 0.0020 s/iter. Inference: 0.1582 s/iter. Eval: 0.0238 s/iter. Total: 0.1842 s/iter. ETA=0:01:25\n","\u001b[32m[01/29 11:14:08 d2.evaluation.evaluator]: \u001b[0mInference done 451/890. Dataloading: 0.0020 s/iter. Inference: 0.1583 s/iter. Eval: 0.0240 s/iter. Total: 0.1844 s/iter. ETA=0:01:20\n","\u001b[32m[01/29 11:14:13 d2.evaluation.evaluator]: \u001b[0mInference done 479/890. Dataloading: 0.0020 s/iter. Inference: 0.1583 s/iter. Eval: 0.0238 s/iter. Total: 0.1843 s/iter. ETA=0:01:15\n","\u001b[32m[01/29 11:14:18 d2.evaluation.evaluator]: \u001b[0mInference done 507/890. Dataloading: 0.0020 s/iter. Inference: 0.1583 s/iter. Eval: 0.0238 s/iter. Total: 0.1842 s/iter. ETA=0:01:10\n","\u001b[32m[01/29 11:14:23 d2.evaluation.evaluator]: \u001b[0mInference done 535/890. Dataloading: 0.0020 s/iter. Inference: 0.1583 s/iter. Eval: 0.0238 s/iter. Total: 0.1842 s/iter. ETA=0:01:05\n","\u001b[32m[01/29 11:14:28 d2.evaluation.evaluator]: \u001b[0mInference done 562/890. Dataloading: 0.0020 s/iter. Inference: 0.1583 s/iter. Eval: 0.0240 s/iter. Total: 0.1844 s/iter. ETA=0:01:00\n","\u001b[32m[01/29 11:14:33 d2.evaluation.evaluator]: \u001b[0mInference done 588/890. Dataloading: 0.0020 s/iter. Inference: 0.1583 s/iter. Eval: 0.0243 s/iter. Total: 0.1847 s/iter. ETA=0:00:55\n","\u001b[32m[01/29 11:14:38 d2.evaluation.evaluator]: \u001b[0mInference done 616/890. Dataloading: 0.0020 s/iter. Inference: 0.1583 s/iter. Eval: 0.0243 s/iter. Total: 0.1846 s/iter. ETA=0:00:50\n","\u001b[32m[01/29 11:14:44 d2.evaluation.evaluator]: \u001b[0mInference done 643/890. Dataloading: 0.0019 s/iter. Inference: 0.1583 s/iter. Eval: 0.0244 s/iter. Total: 0.1848 s/iter. ETA=0:00:45\n","\u001b[32m[01/29 11:14:49 d2.evaluation.evaluator]: \u001b[0mInference done 671/890. Dataloading: 0.0019 s/iter. Inference: 0.1583 s/iter. Eval: 0.0244 s/iter. Total: 0.1847 s/iter. ETA=0:00:40\n","\u001b[32m[01/29 11:14:54 d2.evaluation.evaluator]: \u001b[0mInference done 699/890. Dataloading: 0.0019 s/iter. Inference: 0.1583 s/iter. Eval: 0.0242 s/iter. Total: 0.1845 s/iter. ETA=0:00:35\n","\u001b[32m[01/29 11:14:59 d2.evaluation.evaluator]: \u001b[0mInference done 727/890. Dataloading: 0.0019 s/iter. Inference: 0.1583 s/iter. Eval: 0.0241 s/iter. Total: 0.1844 s/iter. ETA=0:00:30\n","\u001b[32m[01/29 11:15:04 d2.evaluation.evaluator]: \u001b[0mInference done 756/890. Dataloading: 0.0019 s/iter. Inference: 0.1583 s/iter. Eval: 0.0237 s/iter. Total: 0.1841 s/iter. ETA=0:00:24\n","\u001b[32m[01/29 11:15:09 d2.evaluation.evaluator]: \u001b[0mInference done 786/890. Dataloading: 0.0019 s/iter. Inference: 0.1583 s/iter. Eval: 0.0231 s/iter. Total: 0.1835 s/iter. ETA=0:00:19\n","\u001b[32m[01/29 11:15:14 d2.evaluation.evaluator]: \u001b[0mInference done 814/890. Dataloading: 0.0019 s/iter. Inference: 0.1583 s/iter. Eval: 0.0231 s/iter. Total: 0.1834 s/iter. ETA=0:00:13\n","\u001b[32m[01/29 11:15:19 d2.evaluation.evaluator]: \u001b[0mInference done 842/890. Dataloading: 0.0019 s/iter. Inference: 0.1583 s/iter. Eval: 0.0230 s/iter. Total: 0.1834 s/iter. ETA=0:00:08\n","\u001b[32m[01/29 11:15:24 d2.evaluation.evaluator]: \u001b[0mInference done 870/890. Dataloading: 0.0020 s/iter. Inference: 0.1584 s/iter. Eval: 0.0229 s/iter. Total: 0.1833 s/iter. ETA=0:00:03\n","\u001b[32m[01/29 11:15:28 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:02:42.212600 (0.183291 s / iter per device, on 1 devices)\n","\u001b[32m[01/29 11:15:28 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:02:20 (0.158318 s / iter per device, on 1 devices)\n","\u001b[32m[01/29 11:15:28 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n","\u001b[32m[01/29 11:15:28 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n","\u001b[32m[01/29 11:15:28 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n","Loading and preparing results...\n","DONE (t=0.00s)\n","creating index...\n","index created!\n","\u001b[32m[01/29 11:15:28 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n","\u001b[32m[01/29 11:15:29 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.50 seconds.\n","\u001b[32m[01/29 11:15:29 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n","\u001b[32m[01/29 11:15:29 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.06 seconds.\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.049\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.084\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.051\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.036\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.067\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.041\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.067\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.068\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.016\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.043\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.089\n","\u001b[32m[01/29 11:15:29 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n","|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n","|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n","| 4.907 | 8.354  | 5.068  | 1.363 | 3.577 | 6.745 |\n","\u001b[32m[01/29 11:15:29 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n","| category    | AP     | category   | AP    | category   | AP    |\n","|:------------|:-------|:-----------|:------|:-----------|:------|\n","| ego vehicle | 26.461 | person     | 1.739 | rider      | 0.000 |\n","| car         | 35.588 | truck      | 0.000 | bus        | 0.000 |\n","| caravan     | 0.000  | trailer    | 0.000 | train      | 0.000 |\n","| motorcycle  | 0.000  | bicycle    | 0.000 | pickup     | 0.000 |\n","| van         | 0.000  |            |       |            |       |\n","Loading and preparing results...\n","DONE (t=0.08s)\n","creating index...\n","index created!\n","\u001b[32m[01/29 11:15:29 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *segm*\n","\u001b[32m[01/29 11:15:29 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.39 seconds.\n","\u001b[32m[01/29 11:15:29 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n","\u001b[32m[01/29 11:15:29 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.06 seconds.\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.056\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.099\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.058\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.012\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.035\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.080\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.045\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.070\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.071\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.015\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.041\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.094\n","\u001b[32m[01/29 11:15:29 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for segm: \n","|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n","|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n","| 5.610 | 9.923  | 5.766  | 1.163 | 3.451 | 7.953 |\n","\u001b[32m[01/29 11:15:29 d2.evaluation.coco_evaluation]: \u001b[0mPer-category segm AP: \n","| category    | AP     | category   | AP    | category   | AP    |\n","|:------------|:-------|:-----------|:------|:-----------|:------|\n","| ego vehicle | 36.315 | person     | 1.669 | rider      | 0.000 |\n","| car         | 34.941 | truck      | 0.000 | bus        | 0.000 |\n","| caravan     | 0.000  | trailer    | 0.000 | train      | 0.000 |\n","| motorcycle  | 0.000  | bicycle    | 0.000 | pickup     | 0.000 |\n","| van         | 0.000  |            |       |            |       |\n","OrderedDict([('bbox', {'AP': 4.9067096739454925, 'AP50': 8.353547597293, 'AP75': 5.068190039967709, 'APs': 1.3627891694049334, 'APm': 3.577040707910415, 'APl': 6.745464022324342, 'AP-ego vehicle': 26.46068709639132, 'AP-person': 1.73892652423137, 'AP-rider': 0.0, 'AP-car': 35.5876121406687, 'AP-truck': 0.0, 'AP-bus': 0.0, 'AP-caravan': 0.0, 'AP-trailer': 0.0, 'AP-train': 0.0, 'AP-motorcycle': 0.0, 'AP-bicycle': 0.0, 'AP-pickup': 0.0, 'AP-van': 0.0}), ('segm', {'AP': 5.609549508186557, 'AP50': 9.922931157054123, 'AP75': 5.765742832890163, 'APs': 1.1633420830443915, 'APm': 3.4507683060501413, 'APl': 7.952826472260184, 'AP-ego vehicle': 36.31467704440192, 'AP-person': 1.6686080372743157, 'AP-rider': 0.0, 'AP-car': 34.940858524749004, 'AP-truck': 0.0, 'AP-bus': 0.0, 'AP-caravan': 0.0, 'AP-trailer': 0.0, 'AP-train': 0.0, 'AP-motorcycle': 0.0, 'AP-bicycle': 0.0, 'AP-pickup': 0.0, 'AP-van': 0.0})])\n"]}],"source":["from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n","from detectron2.data import build_detection_test_loader\n","\n","evaluator = COCOEvaluator(\"dash_test\", cfg, False, output_dir=\"./output\")\n","val_loader = build_detection_test_loader(cfg, \"dash_test\")\n","print(inference_on_dataset(predictor.model, val_loader, evaluator))"]},{"cell_type":"code","source":["data: List[Dict] = DatasetCatalog.get(\"wildash_test\")"],"metadata":{"id":"n-ZBgqJneamu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MetadataCatalog.get(\"my_dataset\").thing_classes"],"metadata":{"id":"-1sUlgdvgd_T"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hwndWKqoQyjC","executionInfo":{"status":"aborted","timestamp":1643453307614,"user_tz":-60,"elapsed":40,"user":{"displayName":"Matteo De Marchi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04834726655050483734"}}},"outputs":[],"source":["%load_ext tensorboard\n","%tensorboard --logdir output"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["tOxDYIJdhExZ"],"name":"MaskRCNN.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}